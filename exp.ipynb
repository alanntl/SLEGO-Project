{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from typing import Union, Dict, Any, List\n",
    "\n",
    "def __check_module(module_name: str) -> bool:\n",
    "    \"\"\"Private function to check if a Python module is installed.\"\"\"\n",
    "    import importlib.util\n",
    "    return importlib.util.find_spec(module_name) is not None\n",
    "\n",
    "def __ensure_directory_exists(file_path: str) -> None:\n",
    "    \"\"\"Private function to ensure directory exists.\"\"\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def __read_file_content(file_path: str, installed_modules: Dict[str, bool]) -> str:\n",
    "    \"\"\"Private function to read content from various file types.\"\"\"\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        if file_extension == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "                \n",
    "        elif file_extension == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                return json.dumps(data, indent=2)\n",
    "                \n",
    "        elif file_extension == '.docx' and installed_modules.get('docx'):\n",
    "            import docx\n",
    "            doc = docx.Document(file_path)\n",
    "            return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "            \n",
    "        elif file_extension == '.csv' and installed_modules.get('pandas'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            return df.to_string()\n",
    "            \n",
    "        elif file_extension == '.xlsx' and installed_modules.get('pandas'):\n",
    "            df = pd.read_excel(file_path)\n",
    "            return df.to_string()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading {file_extension} file: {str(e)}\")\n",
    "\n",
    "def __analyze_content(client: OpenAI, content: str, filename: str, \n",
    "                     system_role: str, analysis_task: str, \n",
    "                     additional_instructions: str, model: str,\n",
    "                     max_tokens: int, temperature: float, \n",
    "                     chunk_size: int) -> str:\n",
    "    \"\"\"Private function to analyze content using OpenAI API.\"\"\"\n",
    "    try:\n",
    "        # Split content into manageable chunks\n",
    "        content_chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "        full_analysis = []\n",
    "\n",
    "        # Analyze each chunk\n",
    "        for i, chunk in enumerate(content_chunks):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_role},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Task: {analysis_task}\n",
    "Additional Instructions: {additional_instructions}\n",
    "Analyzing file: {filename}\n",
    "Content Part {i+1}/{len(content_chunks)}:\n",
    "\n",
    "{chunk}\n",
    "                \"\"\"}\n",
    "            ]\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            full_analysis.append(response.choices[0].message.content)\n",
    "        \n",
    "        # Consolidate analysis if multiple chunks exist\n",
    "        if len(full_analysis) > 1:\n",
    "            summary_messages = [\n",
    "                {\"role\": \"system\", \"content\": system_role},\n",
    "                {\"role\": \"user\", \"content\": f\"Please provide a consolidated summary of these analyses:\\n\\n{''.join(full_analysis)}\"}\n",
    "            ]\n",
    "            \n",
    "            summary_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=summary_messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            \n",
    "            return summary_response.choices[0].message.content\n",
    "        \n",
    "        return full_analysis[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "def universal_analyzer(\n",
    "    input_path: str = 'dataspace/input/',\n",
    "    output_file: str = 'dataspace/analysis_results.json',\n",
    "    api_key: str = '',\n",
    "    system_role: str = \"You are a highly skilled analyst with expertise in multiple domains.\",\n",
    "    analysis_task: str = \"Analyze this content and provide key insights, patterns, and recommendations.\",\n",
    "    additional_instructions: str = \"\",\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens: int = 2000,\n",
    "    temperature: float = 0.7,\n",
    "    chunk_size: int = 4000\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyzes various file types using OpenAI's GPT models.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_path: Path to file or directory to analyze\n",
    "    - output_file: Where to save analysis results\n",
    "    - api_key: OpenAI API key\n",
    "    - system_role: Role description for the AI\n",
    "    - analysis_task: Main analysis task description\n",
    "    - additional_instructions: Extra analysis instructions\n",
    "    - model: OpenAI model to use\n",
    "    - max_tokens: Maximum tokens in response\n",
    "    - temperature: Response randomness (0-1)\n",
    "    - chunk_size: Maximum characters per API call\n",
    "    \n",
    "    Returns:\n",
    "    - Dict containing analysis results and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize modules and file types\n",
    "        installed_modules = {}\n",
    "        required_modules = ['docx', 'pdfplumber', 'pandas', 'openpyxl']\n",
    "        for module in required_modules:\n",
    "            installed_modules[module] = __check_module(module)\n",
    "\n",
    "        supported_file_types = ['.txt', '.json']  # Add JSON support\n",
    "        if installed_modules.get('docx'):\n",
    "            supported_file_types.append('.docx')\n",
    "        if installed_modules.get('pandas'):\n",
    "            supported_file_types.extend(['.csv', '.xlsx'])\n",
    "\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OpenAI API key is required\")\n",
    "\n",
    "        # Initialize client and results\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        analysis_results = {\n",
    "            'system_info': {\n",
    "                'installed_modules': installed_modules,\n",
    "                'supported_file_types': supported_file_types\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Process single file\n",
    "        if os.path.isfile(input_path):\n",
    "            file_extension = os.path.splitext(input_path)[1].lower()\n",
    "            if file_extension in supported_file_types:\n",
    "                content = __read_file_content(input_path, installed_modules)\n",
    "                analysis = __analyze_content(\n",
    "                    client, content, os.path.basename(input_path),\n",
    "                    system_role, analysis_task, additional_instructions,\n",
    "                    model, max_tokens, temperature, chunk_size\n",
    "                )\n",
    "                analysis_results[os.path.basename(input_path)] = {\n",
    "                    'file_path': input_path,\n",
    "                    'analysis': analysis\n",
    "                }\n",
    "            else:\n",
    "                analysis_results[os.path.basename(input_path)] = {\n",
    "                    'file_path': input_path,\n",
    "                    'error': f\"Unsupported file type: {file_extension}\"\n",
    "                }\n",
    "                \n",
    "        # Process directory\n",
    "        else:\n",
    "            for root, _, files in os.walk(input_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_extension = os.path.splitext(file)[1].lower()\n",
    "                    \n",
    "                    if file_extension in supported_file_types:\n",
    "                        content = __read_file_content(file_path, installed_modules)\n",
    "                        analysis = __analyze_content(\n",
    "                            client, content, file,\n",
    "                            system_role, analysis_task, additional_instructions,\n",
    "                            model, max_tokens, temperature, chunk_size\n",
    "                        )\n",
    "                        analysis_results[file] = {\n",
    "                            'file_path': file_path,\n",
    "                            'analysis': analysis\n",
    "                        }\n",
    "                    else:\n",
    "                        analysis_results[file] = {\n",
    "                            'file_path': file_path,\n",
    "                            'error': f\"Unsupported file type: {file_extension}\"\n",
    "                        }\n",
    "\n",
    "        # Save results\n",
    "        __ensure_directory_exists(output_file)\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analysis_results, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        return analysis_results\n",
    "\n",
    "    except Exception as e:\n",
    "        error_result = {'error': str(e)}\n",
    "        __ensure_directory_exists(output_file)\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(error_result, f, indent=4, ensure_ascii=False)\n",
    "        return error_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the necessary modules installed:\n",
    "# pip install openai pandas docx pdfplumber openpyxl\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "input_path='/Users/an/Library/CloudStorage/OneDrive-Personal/Documents/GitHub/SLEGO-Project/slegospace/dataspace/news.json'\n",
    "\n",
    "# Call the universal_analyzer function\n",
    "results = universal_analyzer(\n",
    "    input_path= input_path,  # Update this path to your input directory or file\n",
    "    output_file='dataspace/analysis_results.json',\n",
    "    api_key=api_key,\n",
    "    analysis_task=\"Provide a summary and key insights from the content.\",\n",
    "    additional_instructions=\"Focus on any financial data and trends.\",\n",
    "    model=\"gpt-4o\",  # Use \"gpt-3.5-turbo\" if you don't have access to gpt-4\n",
    "    max_tokens=1500,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# The analysis results are now saved in 'dataspace/analysis_results.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_info': {'installed_modules': {'docx': True,\n",
       "   'pdfplumber': False,\n",
       "   'pandas': True,\n",
       "   'openpyxl': True},\n",
       "  'supported_file_types': ['.txt', '.json', '.docx', '.csv', '.xlsx']},\n",
       " 'news.json': {'file_path': '/Users/an/Library/CloudStorage/OneDrive-Personal/Documents/GitHub/SLEGO-Project/slegospace/dataspace/news.json',\n",
       "  'analysis': 'The consolidated summary of the analyses from the news.json file highlights several key themes and insights within the financial and technology sectors:\\n\\n1. **AI Investment and Market Dynamics**:\\n   - There\\'s a notable focus on artificial intelligence (AI) investments, with companies like Amazon, Alphabet, Apple, Microsoft, and Nvidia actively increasing their spending in this area. This surge reflects a strategic emphasis on AI as a driver for future innovation and growth. However, there is also a mention of fading initial excitement around AI, suggesting that companies need to demonstrate tangible results to sustain investor interest.\\n\\n2. **Monetization Challenges in Generative AI**:\\n   - Despite advancements in generative AI technologies, companies are facing difficulties in developing effective monetization strategies, particularly in consumer-focused applications. This challenge is significant for tech giants like Google and Microsoft, who are key players in this space.\\n\\n3. **Tech Giants\\' Market Influence**:\\n   - The financial performance and strategic initiatives of major tech companies, often referred to as the \"Mag 7\" (Amazon, Alphabet, Apple, Microsoft, Meta, and Nvidia), are critical for setting market expectations and influencing broader stock market trends. Their earnings reports and moves in AI are closely monitored by investors and analysts.\\n\\n4. **Passive Income and Investment Strategies**:\\n   - There is a growing trend towards passive income-generating investments, such as ETFs, particularly for retirement strategies. This trend indicates a preference for more secure investment options in uncertain economic climates, with companies like JPMorgan and Microsoft playing roles in this market.\\n\\n5. **Potential Market Events and Volatility**:\\n   - An upcoming event on October 31 is anticipated to potentially impact stock market dynamics, suggesting that investors should be prepared for possible volatility or opportunities around this time.\\n\\n6. **Oracle\\'s Investment Potential**:\\n   - Discussion around Oracle highlights interest in its current market position and potential as an investment, especially in comparison to other tech giants like Microsoft.\\n\\nOverall, these insights reflect a period of significant scrutiny and strategic focus within the technology sector, with AI investments being a central theme. The performance and strategic decisions of major tech companies are pivotal in shaping investor confidence and market directions, while challenges in monetization and a shift towards passive income strategies indicate broader economic considerations.'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slegolite310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
