{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Union\n",
    "import importlib.util\n",
    "\n",
    "def check_module(module_name: str) -> bool:\n",
    "    \"\"\"Check if a Python module is installed.\"\"\"\n",
    "    return importlib.util.find_spec(module_name) is not None\n",
    "\n",
    "def universal_analyzer(\n",
    "    input_path: str = 'dataspace/input/',\n",
    "    output_file: str = 'dataspace/analysis_results.json',\n",
    "    api_key: str = '',\n",
    "    system_role: str = \"You are a highly skilled analyst with expertise in multiple domains.\",\n",
    "    analysis_task: str = \"Analyze this content and provide key insights, patterns, and recommendations.\",\n",
    "    additional_instructions: str = \"\",\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens: int = 2000,\n",
    "    temperature: float = 0.7,\n",
    "    chunk_size: int = 4000  # Maximum characters per API call\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    A universal analyzer that can process various file types and provide AI-powered analysis using OpenAI.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path (str): Path to the input file or directory. Defaults to 'dataspace/input/'.\n",
    "    - output_file (str): Path to the output JSON file. Defaults to 'dataspace/analysis_results.json'.\n",
    "    - api_key (str): OpenAI API key. Required.\n",
    "    - system_role (str): System role description for the AI assistant. Defaults to a generic analyst role.\n",
    "    - analysis_task (str): The analysis task to be performed. Defaults to a general analysis prompt.\n",
    "    - additional_instructions (str): Any additional instructions for the AI assistant.\n",
    "    - model (str): The OpenAI model to use. Defaults to 'gpt-4'.\n",
    "    - max_tokens (int): Maximum number of tokens for the AI response. Defaults to 2000.\n",
    "    - temperature (float): Sampling temperature for the AI response. Defaults to 0.7.\n",
    "    - chunk_size (int): Maximum number of characters per API call. Defaults to 4000.\n",
    "\n",
    "    Returns:\n",
    "    - Dict: A dictionary containing the analysis results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for required modules\n",
    "    installed_modules = {}\n",
    "    required_modules = ['docx', 'pdfplumber', 'pandas', 'openpyxl']\n",
    "    for module in required_modules:\n",
    "        installed_modules[module] = check_module(module)\n",
    "\n",
    "    # Define supported file types based on installed modules\n",
    "    supported_file_types = ['.txt']\n",
    "    if installed_modules.get('docx'):\n",
    "        supported_file_types.append('.docx')\n",
    "        import docx\n",
    "    if installed_modules.get('pdfplumber'):\n",
    "        supported_file_types.append('.pdf')\n",
    "        import pdfplumber\n",
    "    if installed_modules.get('pandas'):\n",
    "        supported_file_types.extend(['.csv', '.xlsx'])\n",
    "        import pandas as pd\n",
    "    else:\n",
    "        raise ImportError(\"The 'pandas' module is required for this function.\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OpenAI API key is required.\")\n",
    "\n",
    "    # Initialize OpenAI client\n",
    "    import openai\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    analysis_results = {}\n",
    "\n",
    "    def read_file_content(file_path: str) -> str:\n",
    "        \"\"\"Reads the content of a file based on its extension.\"\"\"\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        if file_extension == '.txt':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        elif file_extension == '.docx' and installed_modules.get('docx'):\n",
    "            doc = docx.Document(file_path)\n",
    "            return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "        elif file_extension == '.pdf' and installed_modules.get('pdfplumber'):\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                pages = [page.extract_text() for page in pdf.pages]\n",
    "                return '\\n'.join(pages)\n",
    "        elif file_extension == '.csv' and installed_modules.get('pandas'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            return df.to_csv(index=False)\n",
    "        elif file_extension == '.xlsx' and installed_modules.get('pandas'):\n",
    "            df = pd.read_excel(file_path)\n",
    "            return df.to_csv(index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type or required module not installed for: {file_extension}\")\n",
    "\n",
    "    def analyze_content(content: str, filename: str) -> str:\n",
    "        \"\"\"Helper function to analyze content using OpenAI API.\"\"\"\n",
    "        try:\n",
    "            # Split content into chunks if it's too long\n",
    "            content_chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "            \n",
    "            full_analysis = []\n",
    "\n",
    "            for i, chunk in enumerate(content_chunks):\n",
    "                # Prepare messages for the AI assistant\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_role},\n",
    "                    {\"role\": \"user\", \"content\": f\"Task: {analysis_task}\\n\\nAdditional Instructions: {additional_instructions}\\n\\nAnalyzing file: {filename}\\n\\nContent Part {i+1}/{len(content_chunks)}:\\n{chunk}\"}\n",
    "                ]\n",
    "                \n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                full_analysis.append(response['choices'][0]['message']['content'])\n",
    "            \n",
    "            # Consolidate analysis if multiple chunks\n",
    "            if len(full_analysis) > 1:\n",
    "                summary_messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_role},\n",
    "                    {\"role\": \"user\", \"content\": f\"Please provide a consolidated summary of all the previous analyses:\\n\\n{''.join(full_analysis)}\"}\n",
    "                ]\n",
    "                \n",
    "                summary_response = openai.ChatCompletion.create(\n",
    "                    model=model,\n",
    "                    messages=summary_messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                \n",
    "                return summary_response['choices'][0]['message']['content']\n",
    "            \n",
    "            return full_analysis[0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "        # Add module availability to results\n",
    "        analysis_results['system_info'] = {\n",
    "            'installed_modules': installed_modules,\n",
    "            'supported_file_types': supported_file_types\n",
    "        }\n",
    "\n",
    "        # Process single file or directory\n",
    "        if os.path.isfile(input_path):\n",
    "            file_extension = os.path.splitext(input_path)[1].lower()\n",
    "            if file_extension in supported_file_types:\n",
    "                content = read_file_content(input_path)\n",
    "                analysis = analyze_content(content, os.path.basename(input_path))\n",
    "                analysis_results[os.path.basename(input_path)] = {\n",
    "                    'file_path': input_path,\n",
    "                    'analysis': analysis\n",
    "                }\n",
    "            else:\n",
    "                analysis_results[os.path.basename(input_path)] = {\n",
    "                    'file_path': input_path,\n",
    "                    'error': f\"Unsupported file type: {file_extension}\"\n",
    "                }\n",
    "        else:\n",
    "            # Process directory\n",
    "            for root, _, files in os.walk(input_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    file_extension = os.path.splitext(file)[1].lower()\n",
    "                    \n",
    "                    if file_extension in supported_file_types:\n",
    "                        content = read_file_content(file_path)\n",
    "                        analysis = analyze_content(content, file)\n",
    "                        analysis_results[file] = {\n",
    "                            'file_path': file_path,\n",
    "                            'analysis': analysis\n",
    "                        }\n",
    "                    else:\n",
    "                        analysis_results[file] = {\n",
    "                            'file_path': file_path,\n",
    "                            'error': f\"Unsupported file type: {file_extension}\"\n",
    "                        }\n",
    "\n",
    "        # Save results to JSON file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analysis_results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        return analysis_results\n",
    "\n",
    "    except Exception as e:\n",
    "        error_result = {'error': str(e)}\n",
    "        \n",
    "        # Save error to JSON file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(error_result, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        return error_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the necessary modules installed:\n",
    "# pip install openai pandas docx pdfplumber openpyxl\n",
    "\n",
    "# Set your OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "input_path='/Users/an/Library/CloudStorage/OneDrive-Personal/Documents/GitHub/SLEGO-Project/slegospace/dataspace/historical_data.csv'\n",
    "\n",
    "# Call the universal_analyzer function\n",
    "results = universal_analyzer(\n",
    "    input_path= input_path,  # Update this path to your input directory or file\n",
    "    output_file='dataspace/analysis_results.json',\n",
    "    api_key=api_key,\n",
    "    analysis_task=\"Provide a summary and key insights from the content.\",\n",
    "    additional_instructions=\"Focus on any financial data and trends.\",\n",
    "    model=\"gpt-4o\",  # Use \"gpt-3.5-turbo\" if you don't have access to gpt-4\n",
    "    max_tokens=1500,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "\n",
    "# The analysis results are now saved in 'dataspace/analysis_results.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system_info': {'installed_modules': {'docx': True,\n",
       "   'pdfplumber': False,\n",
       "   'pandas': True,\n",
       "   'openpyxl': True},\n",
       "  'supported_file_types': ['.txt', '.docx', '.csv', '.xlsx']},\n",
       " 'historical_data.csv': {'file_path': '/Users/an/Library/CloudStorage/OneDrive-Personal/Documents/GitHub/SLEGO-Project/slegospace/dataspace/historical_data.csv',\n",
       "  'analysis': 'Error during analysis: \\n\\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\\n\\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \\n\\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\\n\\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\\n'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slegolite310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
